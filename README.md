# ğŸ“„ PDF RAG Chatbot using Mistral + FAISS + Gradio

An open-source, fully local Retrieval-Augmented Generation (RAG) chatbot that allows you to chat with any PDF using the **Mistral 7B** model for both **embeddings** and **language generation**.

This project runs locally in Google Colab (or any Python environment), using:
- ğŸ§  `llama-cpp-python` to run Mistral 7B in GGUF format
- âš¡ `FAISS` for vector similarity search
- ğŸ” `LangChain` for managing the RAG pipeline
- ğŸ§ª Custom evaluation metrics for each answer
- ğŸ›ï¸ `Gradio` UI to chat interactively

---

## ğŸ§° Features

- ğŸ“¥ Upload and chat with any PDF document
- ğŸ§© Semantic chunking and indexing with FAISS
- ğŸ¤– Local inference using `llama-cpp` and `Mistral 7B`
- âœ… Evaluation of each response with:
  - Groundedness (is the answer supported by the source?)
  - Context relevance (relation between query and retrieved docs)
  - Answer relevance (semantic match with the question)
- ğŸ’¬ Chatbot UI using Gradio (local or public link)
- ğŸ”’ Runs fully offline â€” no API keys or cloud services needed

---

## ğŸš€ Demo Screenshot

![Screenshot](screenshot.png)  
*(Upload your own UI screenshot named `screenshot.png` in the repo.)*

---

## ğŸ“¦ Installation (Colab Recommended)

1. Upload your `mistral-7b-instruct-v0.1.Q4_K_M.gguf` model to Colab or local folder.
2. Clone the repo and run the notebook step-by-step.
3. PDF is uploaded interactively during runtime.

Or use this repo in Google Colab:  
[ğŸ“” Open in Colab](https://colab.research.google.com/)

---

## ğŸ› ï¸ Tech Stack

- `Mistral 7B` (GGUF) via `llama-cpp-python`
- `FAISS` for vector search
- `LangChain` for RAG
- `sentence-transformers` for evaluation
- `Gradio` for UI
- `pypdf`, `nltk`, `rouge-score` for preprocessing & evaluation

---

## ğŸ“ˆ Evaluation Metrics Explained

| Metric            | Description                                                                 |
|-------------------|-----------------------------------------------------------------------------|
| Groundedness      | Measures if the answer is supported by the retrieved source documents       |
| Context Relevance | Checks how well the retrieved context relates to the question               |
| Answer Relevance  | Semantic similarity between the question and generated answer               |

---

## ğŸ“‚ Project Structure

